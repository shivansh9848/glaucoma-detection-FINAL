{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xW6fM8f8rVW0",
        "outputId": "944f1f06-38eb-4c9b-821c-5390b61caf8f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Cloning into 'GLAUCOMA-DETECTION'...\n",
            "remote: Enumerating objects: 563, done.\u001b[K\n",
            "remote: Counting objects: 100% (98/98), done.\u001b[K\n",
            "remote: Compressing objects: 100% (63/63), done.\u001b[K\n",
            "remote: Total 563 (delta 32), reused 97 (delta 31), pack-reused 465\u001b[K\n",
            "Receiving objects: 100% (563/563), 61.73 MiB | 12.18 MiB/s, done.\n",
            "Resolving deltas: 100% (33/33), done.\n",
            "Updating files: 100% (567/567), done.\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/kr-viku/GLAUCOMA-DETECTION.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LLXy9WqDrcht",
        "outputId": "3cf7e263-80ca-4a35-c2ff-387c416fab6c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['test', 'train']\n"
          ]
        }
      ],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "52yXbnyRzw_F",
        "outputId": "42911478-c53e-4f6c-9bb7-0559682555e7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/3\n",
            "15/15 [==============================] - 45s 3s/step - loss: 1.2314 - accuracy: 0.5714 - val_loss: 0.6465 - val_accuracy: 0.6250\n",
            "Epoch 2/3\n",
            "15/15 [==============================] - 39s 3s/step - loss: 0.5982 - accuracy: 0.6593 - val_loss: 0.5942 - val_accuracy: 0.6719\n",
            "Epoch 3/3\n",
            "15/15 [==============================] - 38s 3s/step - loss: 0.5319 - accuracy: 0.7714 - val_loss: 0.5897 - val_accuracy: 0.6719\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Conv2D, MaxPooling2D, Flatten, Dense\n",
        "import os\n",
        "\n",
        "# Define the model\n",
        "classifier = Sequential([\n",
        "    Conv2D(32, (3, 3), input_shape=(256, 256, 3), activation='relu'),\n",
        "    MaxPooling2D(pool_size=(2, 2)),\n",
        "    Conv2D(32, (3, 3), activation='relu'),\n",
        "    MaxPooling2D(pool_size=(2, 2)),\n",
        "    Flatten(),\n",
        "    Dense(units=128, activation='relu'),\n",
        "    Dense(units=1, activation='sigmoid')\n",
        "])\n",
        "\n",
        "# Compile the model\n",
        "classifier.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Define a function to load and preprocess image files\n",
        "def preprocess_image(filename, label):\n",
        "    image = tf.io.read_file(filename)\n",
        "    image = tf.image.decode_jpeg(image, channels=3)\n",
        "    image = tf.image.resize(image, (256, 256))\n",
        "    image = tf.cast(image, tf.float32) / 255.0  # Normalize pixel values to [0, 1]\n",
        "    return image, label\n",
        "\n",
        "# Define paths to the training and testing data directories\n",
        "train_data_dir = '/content/GLAUCOMA-DETECTION/dataset/train'\n",
        "test_data_dir = '/content/GLAUCOMA-DETECTION/dataset/test'\n",
        "\n",
        "# Map class names to numerical labels\n",
        "class_to_label = {'class0': 0, 'class1': 1}\n",
        "\n",
        "# Get a list of all image filenames and their corresponding labels for training data\n",
        "train_filenames = []\n",
        "train_labels = []\n",
        "for class_name in os.listdir(train_data_dir):\n",
        "    class_dir = os.path.join(train_data_dir, class_name)\n",
        "    class_label = class_to_label[class_name]\n",
        "    for fname in os.listdir(class_dir):\n",
        "        train_filenames.append(os.path.join(class_dir, fname))\n",
        "        train_labels.append(class_label)\n",
        "\n",
        "# Get a list of all image filenames and their corresponding labels for testing data\n",
        "test_filenames = []\n",
        "test_labels = []\n",
        "for class_name in os.listdir(test_data_dir):\n",
        "    class_dir = os.path.join(test_data_dir, class_name)\n",
        "    class_label = class_to_label[class_name]\n",
        "    for fname in os.listdir(class_dir):\n",
        "        test_filenames.append(os.path.join(class_dir, fname))\n",
        "        test_labels.append(class_label)\n",
        "\n",
        "# Create TensorFlow datasets from the image filenames and labels\n",
        "train_dataset = tf.data.Dataset.from_tensor_slices((train_filenames, train_labels))\n",
        "test_dataset = tf.data.Dataset.from_tensor_slices((test_filenames, test_labels))\n",
        "\n",
        "# Map the preprocess_image function to the datasets\n",
        "train_dataset = train_dataset.map(preprocess_image, num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
        "test_dataset = test_dataset.map(preprocess_image, num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
        "\n",
        "# Shuffle and batch the datasets\n",
        "batch_size = 32\n",
        "train_dataset = train_dataset.shuffle(buffer_size=1000).batch(batch_size).prefetch(tf.data.experimental.AUTOTUNE)\n",
        "test_dataset = test_dataset.batch(batch_size)\n",
        "\n",
        "# Fit the model to the training data\n",
        "model_info = classifier.fit(\n",
        "    train_dataset,\n",
        "    epochs=3,\n",
        "    validation_data=test_dataset\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qLGNX_KW0Bar"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
